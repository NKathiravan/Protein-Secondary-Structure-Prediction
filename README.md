# üß¨ Protein Secondary Structure Prediction using Transformer & Hybrid Quantum-Classical Models

## üìñ Overview

This project focuses on **predicting the secondary structure of proteins** in **Q3 format** using deep learning. We explore both classical Transformer-based architectures and quantum-enhanced variants to classify each amino acid residue into one of the **3 standard secondary structure classes**.

---

## üß™ Task Description

### üéØ Goal
To classify each amino acid in a protein sequence into one of the **3 secondary structure labels (Q3 classification)**.

---

## üìÇ Dataset Preprocessing

- **Tokenization**: Protein sequences were tokenized character-wise using a custom `CharTokenizer`, mapping each amino acid to a unique integer.
- **Label Encoding**: Secondary structure labels (Q3 format) were encoded using a `label_map` dictionary.
- **Padding & Truncation**:
  - Sequences were padded or truncated to a **fixed length of 350 tokens**.
  - Attention masks were generated to ignore padded tokens during loss computation.
- **Train-Validation Split**: Dataset split 80:20 using PyTorch's `random_split`.

---

## üß† Model Architectures

### 1Ô∏è‚É£ ProtBERT + MLP Model

- **Embedding**: Sequences passed through a **pre-trained ProtBERT** model to obtain contextual amino acid embeddings.
- **MLP Head**: Fully connected layers with decreasing hidden sizes.
- **Output**: Final predictions made via softmax over **3 classes (Q3 format)**.

### 2Ô∏è‚É£ Transformer from Scratch 

- **Input Embedding**: Amino acid tokens mapped to 128-dimensional vectors (`d_model = 128`).
- **Positional Encoding**: Fixed sinusoidal encodings added to retain positional information.
- **Encoder Layer**:
  - **Multi-Head Attention** with 8 heads (`n_heads = 8`)
  - **Feed-Forward Network** with ReLU activation
  - **LayerNorm + Dropout** after attention and FFN
- **Output Layer**: Linear layer ‚Üí softmax over **3 classes**

---

## üî¨ Quantum Hybridization 

We explore replacing classical Transformer components with **quantum-inspired modules**:

| Classical Component            | Quantum Equivalent                 |
|-------------------------------|------------------------------------|
| Input Embedding               | Quantum Encoding (Angle/Amplitude) |
| Self-Attention Mechanism      | Quantum Self-Attention             |
| Feedforward Network           | Quantum Feedforward Layer          |
| Post-Attention Normalization  | Quantum Normalization/Re-Encoding  |

---

## ‚öôÔ∏è Training Configuration

- **Loss Function**: Cross-entropy (per token), padded positions masked using attention masks.
- **Optimizer**: Adam (`lr = 1e-4`)
- **Batch Size**: 32
- **Epochs**: 25
- **Metrics**: Accuracy, Precision, Recall, F1-score on validation set

---

## üìà Results (Prototype Phase)

- The model is currently in prototype phase and is being evaluated for performance.
- Preliminary results show promise for combining contextualized embeddings and quantum layers.

---

## üìå Future Work

- Expand quantum module integration (currently in experimental phase)
- Fine-tune ProtBERT + Transformer on larger datasets
- Explore **Q8 classification alongside Q3**
- Visualize secondary structure predictions with 3D mapping

---

## üõ†Ô∏è Technologies Used

- Python, PyTorch
- HuggingFace Transformers (ProtBERT)
- Qiskit / PennyLane (for quantum extensions)
- NumPy, Scikit-learn, Matplotlib

---

## ü§ù Contributions

Feel free to fork and contribute to improvements or add support for **Q8 classification**.  
All ideas for better hybrid architectures or evaluation techniques are welcome!

---
